import streamlit as st
import pandas as pd
import pydeck as pdk
import os
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import uuid
from geopy.geocoders import Nominatim
from geopy.distance import geodesic

# ============================================
# 1. CONFIGURATION
# ============================================
st.set_page_config(layout="wide", page_title="Observatoire Climatique", page_icon="üåç")

st.title("üåç Observatoire Climatique Multi-Sc√©narios")
st.markdown("---")

DOSSIER = "Donn√©es"

# ============================================
# 2. CHARGEMENT ET TRAITEMENT (Avec M√©tadonn√©es)
# ============================================

# ============================================
# 2. CHARGEMENT ET TRAITEMENT (STRICT ET NETTOY√â)
# ============================================

def lire_metadonnees_et_data(path):
    description_map = {}
    try:
        # 1. Lecture des commentaires (M√©tadonn√©es)
        # on utilise 'latin-1' qui est le standard Windows/Excel fr√©quent pour les accents
        with open(path, 'r', encoding='latin-1') as f:
            for line in f:
                line = line.strip()
                if line.startswith("#"):
                    if ":" in line:
                        parts = line.replace("#", "").split(":", 1)
                        if len(parts) == 2:
                            key = parts[0].strip()
                            val = parts[1].strip()
                            description_map[key] = val
                else:
                    break
        
        # 2. Lecture des donn√©es (Force le s√©parateur ';')
        df = pd.read_csv(
            path, 
            sep=';',           # On force le point-virgule
            engine="python", 
            comment="#", 
            skip_blank_lines=True, 
            encoding='latin-1' # Important pour que 'P√©riode' soit bien lu
        )
        return df, description_map
    except Exception as e:
        # En cas d'√©chec latin-1, on tente utf-8
        try:
             df = pd.read_csv(path, sep=';', engine="python", comment="#", encoding='utf-8')
             return df, description_map
        except:
            return None, {}

@st.cache_data(show_spinner=False)
def charger_donnees_globales(dossier):
    if not os.path.exists(dossier):
        return None, None, {}

    all_dfs = []
    global_descriptions = {}
    
    # Vos colonnes exactes
    id_cols = ["Point", "Contexte", "P√©riode"]
    latlon_cols = ["Latitude", "Longitude"]

    for f in os.listdir(dossier):
        if not f.endswith(".txt"): continue
        
        df, metas = lire_metadonnees_et_data(os.path.join(dossier, f))
        if df is None: continue
        
        global_descriptions.update(metas)

        # --- NETTOYAGE CRITIQUE DES COLONNES ---
        # 1. On supprime les espaces
        df.columns = [c.strip() for c in df.columns]
        # 2. On supprime le BOM (caract√®re invisible \ufeff) souvent pr√©sent au d√©but des fichiers
        df.columns = [c.replace('\ufeff', '') for c in df.columns]
        # 3. On supprime les colonnes vides (ex: si le fichier finit par ;)
        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]

        # V√©rification stricte : si les colonnes cl√©s manquent, on saute ce fichier
        # On v√©rifie si les colonnes requises sont pr√©sentes
        missing_cols = [c for c in id_cols + latlon_cols if c not in df.columns]
        if missing_cols:
            # Petite tentative de sauvetage pour "Periode" sans accent
            if "Periode" in df.columns and "P√©riode" not in df.columns:
                df = df.rename(columns={"Periode": "P√©riode"})
            else:
                # Si vraiment il manque des colonnes, on ignore ce fichier pour ne pas faire planter le tout
                continue

        # Conversion num√©rique
        for c in df.columns:
            if c in latlon_cols:
                df[c] = pd.to_numeric(df[c].astype(str).str.replace(",", "."), errors="coerce")
            elif c not in id_cols:
                df[c] = pd.to_numeric(df[c].astype(str).str.replace(",", "."), errors="coerce")
        
        all_dfs.append(df)

    if not all_dfs: return None, None, {}

    # Agr√©gation
    combined = pd.concat(all_dfs, ignore_index=True)
    
    # On groupe par vos colonnes ID exactes
    agg_dict = {c: "first" for c in combined.columns if c not in id_cols}
    final_df = combined.groupby(id_cols, as_index=False).agg(agg_dict)

    # Calcul des √©chelles globales
    numeric_vars = [c for c in final_df.columns if c not in id_cols + latlon_cols and pd.api.types.is_numeric_dtype(final_df[c])]
    
    global_scales = {}
    for v in numeric_vars:
        vmin = final_df[v].min()
        vmax = final_df[v].max()
        global_scales[v] = (vmin, vmax)

    return final_df, global_scales, global_descriptions


# ============================================
# 3. LOGIQUE APP
# ============================================

data, echelles_globales, descriptions = charger_donnees_globales(DOSSIER)

if data is None:
    st.error("‚ùå Aucune donn√©e trouv√©e. V√©rifiez le dossier 'Donn√©es'.")
    st.stop()

# --- TABLEAU R√âCAPITULATIF ---
with st.expander("üìä Disponibilit√© des variables (Tableau de synth√®se)", expanded=False):
    dispo = data.groupby("Contexte").count()
    vars_cols = [c for c in dispo.columns if c in echelles_globales.keys()]
    dispo = dispo[vars_cols].T 
    dispo_clean = dispo.applymap(lambda x: "‚úÖ" if x > 0 else "‚ùå")
    
    # Ajout d'une colonne description dans le tableau r√©cap
    dispo_clean.insert(0, "Description", [descriptions.get(idx, "") for idx in dispo_clean.index])
    
    st.dataframe(dispo_clean)

# --- SIDEBAR ---
with st.sidebar:
    st.header("üéõÔ∏è Param√®tres")
    
    # Choix Variable AVEC DESCRIPTION
    variables_dispos = sorted(list(echelles_globales.keys()))
    if not variables_dispos:
        st.error("Aucune variable num√©rique d√©tect√©e.")
        st.stop()
    
    # Fonction de formatage pour afficher "CODE : Description" dans le menu
    def format_variable(code):
        desc = descriptions.get(code, "Description inconnue")
        # On coupe si c'est trop long pour la sidebar
        if len(desc) > 50: desc = desc[:47] + "..."
        return f"{code} - {desc}"

    choix_var = st.selectbox("Variable √† analyser", variables_dispos, format_func=format_variable)
    
    # Affichage de la description compl√®te juste en dessous pour √™tre s√ªr
    st.info(f"**D√©finition :** {descriptions.get(choix_var, 'Pas de description disponible')}")

    st.divider()
    
    # Choix Sc√©nario & Horizon
    scenarios = sorted(data["Contexte"].unique())
    choix_scenario = st.selectbox("Sc√©nario (RCP)", scenarios)
    
    df_step1 = data[data["Contexte"] == choix_scenario]
    horizons = sorted(df_step1["P√©riode"].unique())
    choix_horizon = st.selectbox("P√©riode / Horizon", horizons)
    
    st.divider()
    
    # Style Carte
    st.subheader("üé® Apparence")
    styles_map = {
        "Clair": "https://basemaps.cartocdn.com/gl/positron-gl-style/style.json",
        "Sombre": "https://basemaps.cartocdn.com/gl/dark-matter-gl-style/style.json",
        "Voyager": "https://basemaps.cartocdn.com/gl/voyager-gl-style/style.json"
    }
    style_choisi = st.selectbox("Fond de carte", list(styles_map.keys()))
    
    # L√©gende Globale Fixe & Centr√©e sur le Blanc
    vmin_glob, vmax_glob = echelles_globales[choix_var]
    
    # CALCUL DU CENTRE EXACT POUR QUE LE BLANC SOIT AU MILIEU
    milieu = (vmin_glob + vmax_glob) / 2
    
    # Utilisation de TwoSlopeNorm pour forcer le centre
    # Cela garantit que la couleur blanche est exactement √† 'milieu'
    norm_legend = mcolors.TwoSlopeNorm(vmin=vmin_glob, vcenter=milieu, vmax=vmax_glob)
    
    st.caption(f"√âchelle : {choix_var}")
    
    cmap = plt.get_cmap("coolwarm")
    fig, ax = plt.subplots(figsize=(4, 0.4))
    cb = plt.colorbar(plt.cm.ScalarMappable(norm=norm_legend, cmap=cmap), cax=ax, orientation='horizontal')
    cb.outline.set_visible(False)
    ax.set_axis_off()
    st.pyplot(fig)
    st.write(f"Min: **{vmin_glob:.2f}** | Milieu (Blanc): **{milieu:.2f}** | Max: **{vmax_glob:.2f}**")

# --- PR√âPARATION DONN√âES CARTE ---

df_map = df_step1[df_step1["P√©riode"] == choix_horizon].copy()

# S√©curit√© Variable
if choix_var not in df_map.columns or df_map[choix_var].isna().all():
    st.warning(f"‚ö†Ô∏è Donn√©e indisponible : La variable **{choix_var}** n'existe pas pour {choix_scenario} / {choix_horizon}.")
    st.stop()

df_map = df_map.dropna(subset=["Latitude", "Longitude", choix_var])

# --- G√âOCODAGE ---
@st.cache_data(show_spinner=False)
def geocode_safe(address):
    try:
        agent = f"app_climat_{uuid.uuid4()}"
        geolocator = Nominatim(user_agent=agent, timeout=3)
        loc = geolocator.geocode(address)
        if loc: return loc.latitude, loc.longitude
    except: pass
    return None, None

col_search, col_kpi = st.columns([2, 1])

with col_search:
    adr = st.text_input("üìç Rechercher une localisation", placeholder="Ex: Toulouse, France")
    u_lat, u_lon = None, None
    if adr:
        u_lat, u_lon = geocode_safe(adr)
        if not u_lat: st.warning("Adresse introuvable.")

with col_kpi:
    avg_val = df_map[choix_var].mean()
    st.metric(f"Moyenne Nationale ({choix_scenario})", f"{avg_val:.2f}")

# --- RENDU CARTE ---

# Application des couleurs avec le gradient centr√©
# TwoSlopeNorm permet d'assurer que vcenter est le point blanc
norm = mcolors.TwoSlopeNorm(vmin=vmin_glob, vcenter=milieu, vmax=vmax_glob)
rgb = (cmap(norm(df_map[choix_var].values))[:, :3] * 255).astype(int)
df_map["r"], df_map["g"], df_map["b"] = rgb[:, 0], rgb[:, 1], rgb[:, 2]

layers = []

# Calque Pixels (8km)
grid_layer = pdk.Layer(
    "GridCellLayer",
    data=df_map,
    get_position="[Longitude, Latitude]",
    get_color="[r, g, b, 170]",
    cell_size=8000,
    extruded=False,
    pickable=True,
    auto_highlight=True
)
layers.append(grid_layer)

if u_lat:
    user_layer = pdk.Layer(
        "ScatterplotLayer",
        data=pd.DataFrame({"lat": [u_lat], "lon": [u_lon]}),
        get_position="[lon, lat]",
        get_color="[0, 255, 0]",
        get_radius=5000,
        stroked=True,
        get_line_color=[0,0,0],
        line_width_min_pixels=3
    )
    layers.append(user_layer)
    view_state = pdk.ViewState(latitude=u_lat, longitude=u_lon, zoom=9)
else:
    view_state = pdk.ViewState(latitude=46.6, longitude=2.0, zoom=5.5)

st.pydeck_chart(pdk.Deck(
    map_style=styles_map[style_choisi],
    initial_view_state=view_state,
    layers=layers,
    tooltip={"html": f"<b>{choix_var}:</b> {{{choix_var}}}<br><i>Station: {{Point}}</i>"}
))

# --- ANALYSE LOCALE ---

if u_lat:
    st.divider()
    st.subheader("üîç Analyse Locale")
    
    df_map["dist_km"] = df_map.apply(
        lambda r: geodesic((u_lat, u_lon), (r["Latitude"], r["Longitude"])).km, axis=1
    )
    
    voisins = df_map.nsmallest(5, "dist_km")
    
    col_g, col_d = st.columns(2)
    
    with col_g:
        st.info("üìç Pixel le plus proche")
        proche = voisins.iloc[0]
        st.write(f"**Identifiant :** {proche['Point']}")
        st.write(f"**Distance :** {proche['dist_km']:.2f} km")
        st.metric(f"Valeur r√©elle", f"{proche[choix_var]:.2f}")

    with col_d:
        st.success("üßÆ Estimation Interpol√©e")
        weights = 1 / (voisins["dist_km"] + 0.01)**2
        val_est = np.sum(voisins[choix_var] * weights) / np.sum(weights)
        st.metric(f"Valeur estim√©e", f"{val_est:.2f}")

    st.write("---")
    st.write("**D√©tail des donn√©es utilis√©es :**")
    
    cols_to_show = ["Point", choix_var, "dist_km"]
    st.dataframe(voisins[cols_to_show].style.format({choix_var: "{:.2f}", "dist_km": "{:.2f} km"}))
